{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CMPUT 466/566, Winter 2020 Introduction to Machine learning \n",
    "## Coding Assignment 2 \n",
    "### Problem 1 Report\n",
    "By Nathan Klapstein #1449872"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "\n",
    "import struct\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load in the MNIST data for later compute."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [
    "\n",
    "def readMNISTdata():\n",
    "    with open('data/t10k-images-idx3-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "        test_data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        test_data = test_data.reshape((size, nrows * ncols))\n",
    "\n",
    "    with open('data/t10k-labels-idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        test_labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        test_labels = test_labels.reshape((size, 1))\n",
    "\n",
    "    with open('data/train-images-idx3-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "        train_data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        train_data = train_data.reshape((size, nrows * ncols))\n",
    "\n",
    "    with open('data/train-labels-idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        train_labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        train_labels = train_labels.reshape((size, 1))\n",
    "\n",
    "    # augmenting a constant feature of 1 (absorbing the bias term)\n",
    "    train_data = np.concatenate(\n",
    "        (np.ones([train_data.shape[0], 1]), train_data), axis=1)\n",
    "    test_data = np.concatenate((np.ones([test_data.shape[0], 1]), test_data),\n",
    "                               axis=1)\n",
    "    np.random.seed(314)\n",
    "    np.random.shuffle(train_labels)\n",
    "    np.random.seed(314)\n",
    "    np.random.shuffle(train_data)\n",
    "\n",
    "    X_train = train_data[:50000] / 256\n",
    "    t_train = train_labels[:50000]\n",
    "\n",
    "    X_val = train_data[50000:] / 256\n",
    "    t_val = train_labels[50000:]\n",
    "\n",
    "    return X_train, t_train, X_val, t_val, test_data, test_labels\n",
    "\n",
    "\n",
    "X_train, t_train, X_val, t_val, X_test, t_test = readMNISTdata()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Various global configurations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "N_class = 10\n",
    "alpha = 0.4  # learning rate\n",
    "batch_size = 1000000  # batch size\n",
    "MaxIter = 100  # Maximum iteration\n",
    "decay = 0.  # weight decay\n",
    "\n",
    "# TODO: figure this shit out\n",
    "lam = 0.4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "def oneHotIt(Y):\n",
    "    \"\"\"Convert unidimensional array of labels into a one-hot variant\n",
    "    where the array is size m (examples) x n (classes).\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    Y = Y[:,0]\n",
    "    OHX = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n",
    "    OHX = np.array(OHX.todense()).T\n",
    "    return OHX\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n",
    "    return sm\n",
    "\n",
    "\n",
    "def getLoss(w,x,y,lam):\n",
    "    m = x.shape[0] #First we get the number of training examples\n",
    "    y_mat = oneHotIt(y) #Next we convert the integer class coding into a one-hot representation\n",
    "    scores = np.dot(x,w) #Then we compute raw class scores given our input and current weights\n",
    "    prob = softmax(scores) #Next we perform a softmax on these scores to get their probabilities\n",
    "    loss = (-1 / m) * np.sum(y_mat * np.log(prob)) + (lam/2)*np.sum(w*w) #We then find the loss of the probabilities\n",
    "    # print(x.shape)\n",
    "    # print(y.shape)\n",
    "    # print(y_mat.shape)\n",
    "    # print(y_mat)\n",
    "    # print(prob.shape)\n",
    "    # print(y_mat[0])\n",
    "    # print(prob[0])\n",
    "    # print(np.sum(prob[0]))\n",
    "    # print(w.shape)\n",
    "    grad = (-1 / m) * np.dot(x.T, (y_mat - prob)) + lam*w #And compute the gradient for that loss\n",
    "    # print(grad.shape)\n",
    "    return loss, grad\n",
    "\n",
    "\n",
    "\n",
    "def getProbsAndPreds(someX, w):\n",
    "    probs = softmax(np.dot(someX, w))\n",
    "    preds = np.argmax(probs,axis=1)\n",
    "    return probs,preds\n",
    "\n",
    "\n",
    "# inspired by https://medium.com/@awjuliani/simple-softmax-in-python-tutorial-d6b4c4ed5c16\n",
    "# https://www.w3resource.com/numpy/manipulation/ndarray-flatten.php\n",
    "def getAccuracy(someX,someY,w):\n",
    "    prob,prede = getProbsAndPreds(someX,w)\n",
    "    someY = someY.flatten()\n",
    "    num_correct = np.sum(prede == someY)\n",
    "    num_incorrect = np.sum(prede != someY)\n",
    "    accuracy = num_correct/(num_correct+num_incorrect)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def predict(X, W, t=None):\n",
    "    # X_new: Nsample x (d+1)\n",
    "    # W: (d+1) x K\n",
    "    \n",
    "    reg = 0.001\n",
    "    # TODO Your code here\n",
    "    y = np.matmul(X, W)\n",
    "    \n",
    "    # y = np.argmax(X.dot(W), 1)\n",
    "    t_hat = np.argmax(y, axis=1)\n",
    "    num_of_samples = X.shape[0]\n",
    "\n",
    "    z = np.dot(X, W)\n",
    "    z -= np.max(z, axis=0)  # Max trick for the softmax, preventing infinite values\n",
    "    y_prob = np.exp(z) / np.sum(np.exp(z)) \n",
    "    # ~z_i = z_i - z_max_i\n",
    "    # exp_z = np.exp(z - np.max(z, axis=0) + np.min(z, axis=0))\n",
    "    # p = exp_z / np.sum(exp_z, axis=0) # softmax \n",
    "    # p = softmaxEquation(z)\n",
    "     \n",
    "    # compute the loss\n",
    "    loss = -np.log(np.max(y_prob)) * t\n",
    "    reg_loss = 0.5  * reg * np.sum(np.multiply(W, W))  # Regularization term\n",
    "    cross_entropy_loss =  (1 / num_of_samples) * (np.sum(loss)) \n",
    "    total_loss = cross_entropy_loss + reg_loss\n",
    "    # total_loss = cross_entropy_loss\n",
    "    \n",
    "    # compute the mean-per class accuracy\n",
    "    acc = getAccuracy(X, t, W)\n",
    "    \n",
    "    # Calculation of dW (gradient)\n",
    "    grad = ((-1 / num_of_samples) * np.dot(X.T, (t - y_prob))) + (reg * W)\n",
    "    \n",
    "    return y, t_hat, total_loss, acc, grad\n",
    "\n",
    "\n",
    "def train(X_train, t_train, X_val, t_val):\n",
    "    N_train = X_train.shape[0]\n",
    "    N_val = X_val.shape[0]\n",
    "\n",
    "    # TODO Your code here\n",
    "    \n",
    "    # initialization\n",
    "    w = np.zeros([X_train.shape[1], N_class])\n",
    "    # w: (d+1)x1\n",
    "\n",
    "    losses_train = []\n",
    "    acc_val = []\n",
    "    w_best = None\n",
    "    acc_best = 0\n",
    "    epoch_best = 0\n",
    "    \n",
    "    for epoch in range(MaxIter):\n",
    "        loss_this_epoch = 0\n",
    "        for b in range(int(np.ceil(N_train / batch_size))):\n",
    "            X_batch = X_train[b * batch_size: (b + 1) * batch_size]\n",
    "            t_batch = t_train[b * batch_size: (b + 1) * batch_size]\n",
    "            loss_batch, grad = getLoss(w, X_batch, t_batch, lam)\n",
    "            loss_this_epoch += loss_batch\n",
    "            w = w - (alpha * grad)\n",
    "   \n",
    "        # TODO: Your code here\n",
    "        # monitor model behavior after each epoch\n",
    "        \n",
    "        # 1. Compute the training loss by averaging loss_this_epoch\n",
    "        #         losses_train.append(loss_this_epoch/int(np.ceil(N_train/batch_size)))\n",
    "        \n",
    "    \n",
    "        loss_this_epoch = loss_this_epoch / (int(np.ceil(batch_size)))\n",
    "        val_acc = getAccuracy(X_val, t_val, w)\n",
    "        print(f\"epoch {epoch}/{MaxIter} loss: {loss_this_epoch} val_acc:{val_acc}\")\n",
    "        \n",
    "        # test_acc = getAccuracy(X_train, t_train, w)\n",
    "        # print(f\"est_acc:{test_acc} val_acc:{val_acc}\")\n",
    "        losses_train.append(loss_this_epoch)\n",
    "        #print(f\"epoch={epoch} training loss={loss_this_epoch/  (int(np.ceil(N_train/batch_size)))}\")\n",
    "\n",
    "        # 2. Perform validation on the validation test by the risk\n",
    "        # risk_this_epoch = 0\n",
    "        # for c in range(int(np.ceil(N_val / batch_size))):\n",
    "        #     X_val_batch = X_val[c * batch_size: (c + 1) * batch_size]\n",
    "        #     y_val_batch = y_val[c * batch_size: (c + 1) * batch_size]\n",
    "        #     _, _, risk = predict(X_val_batch, w, y_val_batch)\n",
    "        #     risk_this_epoch += risk\n",
    "        # TODO: FIXED: Validation set should not be run in batches\n",
    "        # y, t_hat, _, acc, grad = predict(X_val, w, t_val)\n",
    "        acc_val.append(val_acc)\n",
    "        \n",
    "        # 3. Keep track of the best validation epoch, risk, and the weights\n",
    "        if acc_val[epoch] >= acc_best:\n",
    "            epoch_best = epoch\n",
    "            acc_best = acc_val[epoch]\n",
    "            w_best = w\n",
    "\n",
    "    # Return some variables as needed\n",
    "    return epoch_best, losses_train, acc_val, acc_best, w_best\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(50000, 785) (50000, 1) (10000, 785) (10000, 1) (10000, 785) (10000, 1)\n",
      "epoch 0/100 loss: 2.3025850929940443e-06 val_acc:0.665\n",
      "epoch 1/100 loss: 1.951664044804229e-06 val_acc:0.7719\n",
      "epoch 2/100 loss: 1.7791795022920606e-06 val_acc:0.7714\n",
      "epoch 3/100 loss: 1.6959260212888437e-06 val_acc:0.7928\n",
      "epoch 4/100 loss: 1.6551098477575747e-06 val_acc:0.7947\n",
      "epoch 5/100 loss: 1.6346090434098395e-06 val_acc:0.8026\n",
      "epoch 6/100 loss: 1.624113141115669e-06 val_acc:0.8051\n",
      "epoch 7/100 loss: 1.6186032515847664e-06 val_acc:0.8087\n",
      "epoch 8/100 loss: 1.6156542514547504e-06 val_acc:0.8104\n",
      "epoch 9/100 loss: 1.6140390914197145e-06 val_acc:0.8125\n",
      "epoch 10/100 loss: 1.613137331664936e-06 val_acc:0.8135\n",
      "epoch 11/100 loss: 1.6126230537501062e-06 val_acc:0.8145\n",
      "epoch 12/100 loss: 1.6123244823052373e-06 val_acc:0.8157\n",
      "epoch 13/100 loss: 1.6121478547590488e-06 val_acc:0.8161\n",
      "epoch 14/100 loss: 1.612041766496415e-06 val_acc:0.8168\n",
      "epoch 15/100 loss: 1.611977047678842e-06 val_acc:0.8174\n",
      "epoch 16/100 loss: 1.6119370897967837e-06 val_acc:0.818\n",
      "epoch 17/100 loss: 1.6119121177245892e-06 val_acc:0.8181\n",
      "epoch 18/100 loss: 1.611896370279385e-06 val_acc:0.8186\n",
      "epoch 19/100 loss: 1.6118863483953785e-06 val_acc:0.8187\n",
      "epoch 20/100 loss: 1.6118799282271111e-06 val_acc:0.8192\n",
      "epoch 21/100 loss: 1.6118757871646076e-06 val_acc:0.8193\n",
      "epoch 22/100 loss: 1.611873103211103e-06 val_acc:0.8193\n",
      "epoch 23/100 loss: 1.6118713547165584e-06 val_acc:0.8193\n",
      "epoch 24/100 loss: 1.6118702115001776e-06 val_acc:0.8197\n",
      "epoch 25/100 loss: 1.6118694610968176e-06 val_acc:0.8195\n",
      "epoch 26/100 loss: 1.6118689671452134e-06 val_acc:0.8198\n",
      "epoch 27/100 loss: 1.611868641000408e-06 val_acc:0.8197\n",
      "epoch 28/100 loss: 1.6118684251646362e-06 val_acc:0.8199\n",
      "epoch 29/100 loss: 1.611868281973504e-06 val_acc:0.8199\n",
      "epoch 30/100 loss: 1.6118681867961127e-06 val_acc:0.82\n",
      "epoch 31/100 loss: 1.6118681234021232e-06 val_acc:0.82\n",
      "epoch 32/100 loss: 1.611868081108695e-06 val_acc:0.82\n",
      "epoch 33/100 loss: 1.6118680528429351e-06 val_acc:0.82\n",
      "epoch 34/100 loss: 1.6118680339249542e-06 val_acc:0.82\n",
      "epoch 35/100 loss: 1.6118680212440066e-06 val_acc:0.82\n",
      "epoch 36/100 loss: 1.611868012732812e-06 val_acc:0.82\n",
      "epoch 37/100 loss: 1.6118680070125624e-06 val_acc:0.82\n",
      "epoch 38/100 loss: 1.611868003163544e-06 val_acc:0.82\n",
      "epoch 39/100 loss: 1.6118680005705018e-06 val_acc:0.82\n",
      "epoch 40/100 loss: 1.6118679988217125e-06 val_acc:0.82\n",
      "epoch 41/100 loss: 1.6118679976410096e-06 val_acc:0.82\n",
      "epoch 42/100 loss: 1.6118679968430594e-06 val_acc:0.82\n",
      "epoch 43/100 loss: 1.6118679963032436e-06 val_acc:0.82\n",
      "epoch 44/100 loss: 1.6118679959377189e-06 val_acc:0.82\n",
      "epoch 45/100 loss: 1.6118679956899848e-06 val_acc:0.82\n",
      "epoch 46/100 loss: 1.611867995521937e-06 val_acc:0.82\n",
      "epoch 47/100 loss: 1.6118679954078474e-06 val_acc:0.82\n",
      "epoch 48/100 loss: 1.611867995330328e-06 val_acc:0.82\n",
      "epoch 49/100 loss: 1.6118679952776153e-06 val_acc:0.82\n",
      "epoch 50/100 loss: 1.6118679952417437e-06 val_acc:0.82\n",
      "epoch 51/100 loss: 1.6118679952173144e-06 val_acc:0.82\n",
      "epoch 52/100 loss: 1.611867995200666e-06 val_acc:0.82\n",
      "epoch 53/100 loss: 1.611867995189312e-06 val_acc:0.82\n",
      "epoch 54/100 loss: 1.6118679951815645e-06 val_acc:0.82\n",
      "epoch 55/100 loss: 1.6118679951762733e-06 val_acc:0.82\n",
      "epoch 56/100 loss: 1.6118679951726582e-06 val_acc:0.82\n",
      "epoch 57/100 loss: 1.611867995170186e-06 val_acc:0.82\n",
      "epoch 58/100 loss: 1.6118679951684952e-06 val_acc:0.82\n",
      "epoch 59/100 loss: 1.6118679951673375e-06 val_acc:0.82\n",
      "epoch 60/100 loss: 1.6118679951665453e-06 val_acc:0.82\n",
      "epoch 61/100 loss: 1.611867995166002e-06 val_acc:0.82\n",
      "epoch 62/100 loss: 1.6118679951656295e-06 val_acc:0.82\n",
      "epoch 63/100 loss: 1.611867995165374e-06 val_acc:0.82\n",
      "epoch 64/100 loss: 1.6118679951651986e-06 val_acc:0.82\n",
      "epoch 65/100 loss: 1.6118679951650787e-06 val_acc:0.82\n",
      "epoch 66/100 loss: 1.6118679951649953e-06 val_acc:0.82\n",
      "epoch 67/100 loss: 1.6118679951649383e-06 val_acc:0.82\n",
      "epoch 68/100 loss: 1.6118679951648996e-06 val_acc:0.82\n",
      "epoch 69/100 loss: 1.6118679951648729e-06 val_acc:0.82\n",
      "epoch 70/100 loss: 1.6118679951648549e-06 val_acc:0.82\n",
      "epoch 71/100 loss: 1.6118679951648417e-06 val_acc:0.82\n",
      "epoch 72/100 loss: 1.6118679951648326e-06 val_acc:0.82\n",
      "epoch 73/100 loss: 1.6118679951648267e-06 val_acc:0.82\n",
      "epoch 74/100 loss: 1.6118679951648227e-06 val_acc:0.82\n",
      "epoch 75/100 loss: 1.6118679951648193e-06 val_acc:0.82\n",
      "epoch 76/100 loss: 1.6118679951648174e-06 val_acc:0.82\n",
      "epoch 77/100 loss: 1.6118679951648165e-06 val_acc:0.82\n",
      "epoch 78/100 loss: 1.611867995164815e-06 val_acc:0.82\n",
      "epoch 79/100 loss: 1.6118679951648148e-06 val_acc:0.82\n",
      "epoch 80/100 loss: 1.6118679951648142e-06 val_acc:0.82\n",
      "epoch 81/100 loss: 1.6118679951648138e-06 val_acc:0.82\n",
      "epoch 82/100 loss: 1.611867995164814e-06 val_acc:0.82\n",
      "epoch 83/100 loss: 1.6118679951648134e-06 val_acc:0.82\n",
      "epoch 84/100 loss: 1.6118679951648136e-06 val_acc:0.82\n",
      "epoch 85/100 loss: 1.611867995164814e-06 val_acc:0.82\n",
      "epoch 86/100 loss: 1.611867995164813e-06 val_acc:0.82\n",
      "epoch 87/100 loss: 1.6118679951648134e-06 val_acc:0.82\n",
      "epoch 88/100 loss: 1.611867995164813e-06 val_acc:0.82\n",
      "epoch 89/100 loss: 1.6118679951648134e-06 val_acc:0.82\n",
      "epoch 90/100 loss: 1.6118679951648132e-06 val_acc:0.82\n",
      "epoch 91/100 loss: 1.611867995164813e-06 val_acc:0.82\n",
      "epoch 92/100 loss: 1.6118679951648134e-06 val_acc:0.82\n",
      "epoch 93/100 loss: 1.6118679951648132e-06 val_acc:0.82\n",
      "epoch 94/100 loss: 1.6118679951648134e-06 val_acc:0.82\n",
      "epoch 95/100 loss: 1.611867995164813e-06 val_acc:0.82\n",
      "epoch 96/100 loss: 1.6118679951648132e-06 val_acc:0.82\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(X_train.shape, t_train.shape, X_val.shape, t_val.shape, X_test.shape,\n",
    "      t_test.shape)\n",
    "\n",
    "\n",
    "epoch_best, losses_train, acc_val, acc_best, W_best = train(X_train, t_train, X_val, t_val)\n",
    "\n",
    "\n",
    "print(f\"At epoch: {epoch_best} acc:{acc_best}\")\n",
    "print(f\"The accuracy best epoch with val set is {acc_best:10.10f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The learning curve of the training loss, where x-axis is the number of epochs,\n",
    "and y-axis is the training loss."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"# part a.plot.1\n",
    "fig = plt.figure()\n",
    "plt.plot(losses_train, label=\"Training Losses\")\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()\n",
    "plt.xlabel('Number of epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Doing predictions on the test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, _, _, acc_best, _ = predict(X_test, W_best, t_test)\n",
    "\n",
    "print(f\"At epoch: {epoch_best} acc:{acc_best}\")\n",
    "\n",
    "print(f\"train accuracy:{getAccuracy(X_train, t_train, W_best)}\")\n",
    "print(f\"validation accuracy:{getAccuracy(X_val, t_val, W_best)}\")\n",
    "print(f\"test accuracy:{getAccuracy(X_test, t_test, W_best)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}