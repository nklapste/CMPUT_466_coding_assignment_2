{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CMPUT 466/566, Winter 2020 Introduction to Machine learning \n",
    "## Coding Assignment 2 \n",
    "### Problem 1 Report\n",
    "By Nathan Klapstein #1449872"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "\n",
    "import struct\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.special import expit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load in the MNIST data for later compute."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m45\u001b[0m\n\u001b[1;33m    X_train, t_train, X_val, t_val, X_test, t_test = readMNISTdata()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ],
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 45)",
     "output_type": "error"
    }
   ],
   "source": [
    "def readMNISTdata():\n",
    "    with open('data/t10k-images-idx3-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "        test_data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        test_data = test_data.reshape((size, nrows * ncols))\n",
    "\n",
    "    with open('data/t10k-labels-idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        test_labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        test_labels = test_labels.reshape((size, 1))\n",
    "\n",
    "    with open('data/train-images-idx3-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "        train_data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        train_data = train_data.reshape((size, nrows * ncols))\n",
    "\n",
    "    with open('data/train-labels-idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        train_labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        train_labels = train_labels.reshape((size, 1))\n",
    "\n",
    "    # augmenting a constant feature of 1 (absorbing the bias term)\n",
    "    train_data = np.concatenate(\n",
    "        (np.ones([train_data.shape[0], 1]), train_data), axis=1)\n",
    "    test_data = np.concatenate((np.ones([test_data.shape[0], 1]), test_data),\n",
    "                               axis=1)\n",
    "    np.random.seed(314)\n",
    "    np.random.shuffle(train_labels)\n",
    "    np.random.seed(314)\n",
    "    np.random.shuffle(train_data)\n",
    "\n",
    "    X_train = train_data[:50000] / 256\n",
    "    t_train = train_labels[:50000]\n",
    "\n",
    "    X_val = train_data[50000:] / 256\n",
    "    t_val = train_labels[50000:]\n",
    "\n",
    "    return X_train, t_train, X_val, t_val, test_data, test_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " X_train, t_train, X_val, t_val, X_test, t_test = readMNISTdata()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Various global configurations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "N_class = 10\n",
    "alpha = 0.4  # learning rate\n",
    "batch_size = 1000000  # batch size\n",
    "MaxIter = 100  # Maximum iteration\n",
    "decay = 0.  # weight decay\n",
    "\n",
    "# TODO: figure this shit out\n",
    "lam = 0.4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def oneHotIt(Y):\n",
    "    \"\"\"Convert unidimensional array of labels into a one-hot variant\n",
    "    where the array is size m (examples) x n (classes).\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    Y = Y[:,0]\n",
    "    OHX = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n",
    "    OHX = np.array(OHX.todense()).T\n",
    "    return OHX\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n",
    "    return sm\n",
    "\n",
    "\n",
    "def getLoss(w,x,y,lam):\n",
    "    m = x.shape[0] #First we get the number of training examples\n",
    "    y_mat = oneHotIt(y) #Next we convert the integer class coding into a one-hot representation\n",
    "    scores = np.dot(x,w) #Then we compute raw class scores given our input and current weights\n",
    "    prob = softmax(scores) #Next we perform a softmax on these scores to get their probabilities\n",
    "    loss = (-1 / m) * np.sum(y_mat * np.log(prob)) + (lam/2)*np.sum(w*w) #We then find the loss of the probabilities\n",
    "    # print(x.shape)\n",
    "    # print(y.shape)\n",
    "    # print(y_mat.shape)\n",
    "    # print(y_mat)\n",
    "    # print(prob.shape)\n",
    "    # print(y_mat[0])\n",
    "    # print(prob[0])\n",
    "    # print(np.sum(prob[0]))\n",
    "    # print(w.shape)\n",
    "    grad = (-1 / m) * np.dot(x.T, (y_mat - prob)) + lam*w #And compute the gradient for that loss\n",
    "    # print(grad.shape)\n",
    "    return loss, grad\n",
    "\n",
    "\n",
    "\n",
    "def getProbsAndPreds(someX, w):\n",
    "    probs = softmax(np.dot(someX, w))\n",
    "    preds = np.argmax(probs,axis=1)\n",
    "    return probs,preds\n",
    "\n",
    "\n",
    "# inspired by https://medium.com/@awjuliani/simple-softmax-in-python-tutorial-d6b4c4ed5c16\n",
    "# https://www.w3resource.com/numpy/manipulation/ndarray-flatten.php\n",
    "def getAccuracy(someX,someY,w):\n",
    "    prob,prede = getProbsAndPreds(someX,w)\n",
    "    someY = someY.flatten()\n",
    "    num_correct = np.sum(prede == someY)\n",
    "    num_incorrect = np.sum(prede != someY)\n",
    "    accuracy = num_correct/(num_correct+num_incorrect)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def predict(X, W, t=None):\n",
    "    # X_new: Nsample x (d+1)\n",
    "    # W: (d+1) x K\n",
    "    \n",
    "    reg = 0.001\n",
    "    # TODO Your code here\n",
    "    y = np.matmul(X, W)\n",
    "    \n",
    "    # y = np.argmax(X.dot(W), 1)\n",
    "    t_hat = np.argmax(y, axis=1)\n",
    "    num_of_samples = X.shape[0]\n",
    "\n",
    "    z = np.dot(X, W)\n",
    "    z -= np.max(z, axis=0)  # Max trick for the softmax, preventing infinite values\n",
    "    y_prob = np.exp(z) / np.sum(np.exp(z)) \n",
    "    # ~z_i = z_i - z_max_i\n",
    "    # exp_z = np.exp(z - np.max(z, axis=0) + np.min(z, axis=0))\n",
    "    # p = exp_z / np.sum(exp_z, axis=0) # softmax \n",
    "    # p = softmaxEquation(z)\n",
    "     \n",
    "    # compute the loss\n",
    "    loss = -np.log(np.max(y_prob)) * t\n",
    "    reg_loss = 0.5  * reg * np.sum(np.multiply(W, W))  # Regularization term\n",
    "    cross_entropy_loss =  (1 / num_of_samples) * (np.sum(loss)) \n",
    "    total_loss = cross_entropy_loss + reg_loss\n",
    "    # total_loss = cross_entropy_loss\n",
    "    \n",
    "    # compute the mean-per class accuracy\n",
    "    acc = getAccuracy(X, t, W)\n",
    "    \n",
    "    # Calculation of dW (gradient)\n",
    "    grad = ((-1 / num_of_samples) * np.dot(X.T, (t - y_prob))) + (reg * W)\n",
    "    \n",
    "    \n",
    "    return y, t_hat, total_loss, acc, grad\n",
    "\n",
    "\n",
    "\n",
    "def train(X_train, t_train, X_val, t_val):\n",
    "    N_train = X_train.shape[0]\n",
    "    N_val = X_val.shape[0]\n",
    "\n",
    "    # TODO Your code here\n",
    "    \n",
    "    # initialization\n",
    "    w = np.zeros([X_train.shape[1], N_class])\n",
    "    # w: (d+1)x1\n",
    "\n",
    "    losses_train = []\n",
    "    acc_val = []\n",
    "    w_best = None\n",
    "    acc_best = 0\n",
    "    epoch_best = 0\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(MaxIter):\n",
    "        loss_this_epoch = 0\n",
    "        for b in range(int(np.ceil(N_train / batch_size))):\n",
    "            X_batch = X_train[b * batch_size: (b + 1) * batch_size]\n",
    "            t_batch = t_train[b * batch_size: (b + 1) * batch_size]\n",
    "            loss_batch, grad = getLoss(w, X_batch, t_batch, lam)\n",
    "            loss_this_epoch += loss_batch\n",
    "            # TODO: grad is wrong shape (785,10) when it should be (785,1)\n",
    "            if b ==5:\n",
    "                return\n",
    "            w = w - (alpha * grad)\n",
    "            # y_hat_batch, t_hat, loss_batch, acc, grad = predict(X_batch, w, t_batch)\n",
    "            # # print(loss_batch)\n",
    "            # loss_this_epoch += loss_batch\n",
    "            # # TODO: Your code here\n",
    "            # # Mini-batch gradient descent\n",
    "            # #TODO: FIX: scale gradient with 1/M\n",
    "            # # w = w - alpha * gradient(X_batch, w, y_hat_batch)\n",
    "            # w -= alpha * grad\n",
    "   \n",
    "        # TODO: Your code here\n",
    "        # monitor model behavior after each epoch\n",
    "        \n",
    "        # 1. Compute the training loss by averaging loss_this_epoch\n",
    "        #         losses_train.append(loss_this_epoch/int(np.ceil(N_train/batch_size)))\n",
    "        \n",
    "    \n",
    "        loss_this_epoch = loss_this_epoch / (int(np.ceil(batch_size)))\n",
    "        val_acc = getAccuracy(X_val, t_val, w)\n",
    "        print(f\"epoch {epoch}/{MaxIter} loss: {loss_this_epoch} val_acc:{val_acc}\")\n",
    "        \n",
    "        # test_acc = getAccuracy(X_train, t_train, w)\n",
    "        # print(f\"est_acc:{test_acc} val_acc:{val_acc}\")\n",
    "        losses_train.append(loss_this_epoch)\n",
    "        #print(f\"epoch={epoch} training loss={loss_this_epoch/  (int(np.ceil(N_train/batch_size)))}\")\n",
    "\n",
    "        # 2. Perform validation on the validation test by the risk\n",
    "        # risk_this_epoch = 0\n",
    "        # for c in range(int(np.ceil(N_val / batch_size))):\n",
    "        #     X_val_batch = X_val[c * batch_size: (c + 1) * batch_size]\n",
    "        #     y_val_batch = y_val[c * batch_size: (c + 1) * batch_size]\n",
    "        #     _, _, risk = predict(X_val_batch, w, y_val_batch)\n",
    "        #     risk_this_epoch += risk\n",
    "        # TODO: FIXED: Validation set should not be run in batches\n",
    "        # y, t_hat, _, acc, grad = predict(X_val, w, t_val)\n",
    "        acc_val.append(val_acc)\n",
    "        \n",
    "        # 3. Keep track of the best validation epoch, risk, and the weights\n",
    "        if acc_val[epoch] >= acc_best:\n",
    "            epoch_best = epoch\n",
    "            acc_best = acc_val[epoch]\n",
    "            w_best = w\n",
    "\n",
    "    # Return some variables as needed\n",
    "    return epoch_best, losses_train, acc_val, acc_best, w_best\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(50000, 785) (50000, 1) (10000, 785) (10000, 1) (10000, 785) (10000, 1)\n",
      "epoch 0/100 loss: 2.3025850929940443e-06 val_acc:0.665\n",
      "epoch 1/100 loss: 1.951664044804229e-06 val_acc:0.7719\n",
      "epoch 2/100 loss: 1.7791795022920606e-06 val_acc:0.7714\n",
      "epoch 3/100 loss: 1.6959260212888437e-06 val_acc:0.7928\n",
      "epoch 4/100 loss: 1.6551098477575747e-06 val_acc:0.7947\n",
      "epoch 5/100 loss: 1.6346090434098395e-06 val_acc:0.8026\n",
      "epoch 6/100 loss: 1.624113141115669e-06 val_acc:0.8051\n",
      "epoch 7/100 loss: 1.6186032515847664e-06 val_acc:0.8087\n",
      "epoch 8/100 loss: 1.6156542514547504e-06 val_acc:0.8104\n",
      "epoch 9/100 loss: 1.6140390914197145e-06 val_acc:0.8125\n",
      "epoch 10/100 loss: 1.613137331664936e-06 val_acc:0.8135\n",
      "epoch 11/100 loss: 1.6126230537501062e-06 val_acc:0.8145\n",
      "epoch 12/100 loss: 1.6123244823052373e-06 val_acc:0.8157\n",
      "epoch 13/100 loss: 1.6121478547590488e-06 val_acc:0.8161\n",
      "epoch 14/100 loss: 1.612041766496415e-06 val_acc:0.8168\n",
      "epoch 15/100 loss: 1.611977047678842e-06 val_acc:0.8174\n",
      "epoch 16/100 loss: 1.6119370897967837e-06 val_acc:0.818\n",
      "epoch 17/100 loss: 1.6119121177245892e-06 val_acc:0.8181\n",
      "epoch 18/100 loss: 1.611896370279385e-06 val_acc:0.8186\n",
      "epoch 19/100 loss: 1.6118863483953785e-06 val_acc:0.8187\n",
      "epoch 20/100 loss: 1.6118799282271111e-06 val_acc:0.8192\n",
      "epoch 21/100 loss: 1.6118757871646076e-06 val_acc:0.8193\n",
      "epoch 22/100 loss: 1.611873103211103e-06 val_acc:0.8193\n",
      "epoch 23/100 loss: 1.6118713547165584e-06 val_acc:0.8193\n",
      "epoch 24/100 loss: 1.6118702115001776e-06 val_acc:0.8197\n",
      "epoch 25/100 loss: 1.6118694610968176e-06 val_acc:0.8195\n",
      "epoch 26/100 loss: 1.6118689671452134e-06 val_acc:0.8198\n",
      "epoch 27/100 loss: 1.611868641000408e-06 val_acc:0.8197\n",
      "epoch 28/100 loss: 1.6118684251646362e-06 val_acc:0.8199\n",
      "epoch 29/100 loss: 1.611868281973504e-06 val_acc:0.8199\n",
      "epoch 30/100 loss: 1.6118681867961127e-06 val_acc:0.82\n",
      "epoch 31/100 loss: 1.6118681234021232e-06 val_acc:0.82\n",
      "epoch 32/100 loss: 1.611868081108695e-06 val_acc:0.82\n",
      "epoch 33/100 loss: 1.6118680528429351e-06 val_acc:0.82\n",
      "epoch 34/100 loss: 1.6118680339249542e-06 val_acc:0.82\n",
      "epoch 35/100 loss: 1.6118680212440066e-06 val_acc:0.82\n",
      "epoch 36/100 loss: 1.611868012732812e-06 val_acc:0.82\n",
      "epoch 37/100 loss: 1.6118680070125624e-06 val_acc:0.82\n",
      "epoch 38/100 loss: 1.611868003163544e-06 val_acc:0.82\n",
      "epoch 39/100 loss: 1.6118680005705018e-06 val_acc:0.82\n",
      "epoch 40/100 loss: 1.6118679988217125e-06 val_acc:0.82\n",
      "epoch 41/100 loss: 1.6118679976410096e-06 val_acc:0.82\n",
      "epoch 42/100 loss: 1.6118679968430594e-06 val_acc:0.82\n",
      "epoch 43/100 loss: 1.6118679963032436e-06 val_acc:0.82\n",
      "epoch 44/100 loss: 1.6118679959377189e-06 val_acc:0.82\n",
      "epoch 45/100 loss: 1.6118679956899848e-06 val_acc:0.82\n",
      "epoch 46/100 loss: 1.611867995521937e-06 val_acc:0.82\n",
      "epoch 47/100 loss: 1.6118679954078474e-06 val_acc:0.82\n",
      "epoch 48/100 loss: 1.611867995330328e-06 val_acc:0.82\n",
      "epoch 49/100 loss: 1.6118679952776153e-06 val_acc:0.82\n",
      "epoch 50/100 loss: 1.6118679952417437e-06 val_acc:0.82\n",
      "epoch 51/100 loss: 1.6118679952173144e-06 val_acc:0.82\n",
      "epoch 52/100 loss: 1.611867995200666e-06 val_acc:0.82\n",
      "epoch 53/100 loss: 1.611867995189312e-06 val_acc:0.82\n",
      "epoch 54/100 loss: 1.6118679951815645e-06 val_acc:0.82\n",
      "epoch 55/100 loss: 1.6118679951762733e-06 val_acc:0.82\n",
      "epoch 56/100 loss: 1.6118679951726582e-06 val_acc:0.82\n",
      "epoch 57/100 loss: 1.611867995170186e-06 val_acc:0.82\n",
      "epoch 58/100 loss: 1.6118679951684952e-06 val_acc:0.82\n",
      "epoch 59/100 loss: 1.6118679951673375e-06 val_acc:0.82\n",
      "epoch 60/100 loss: 1.6118679951665453e-06 val_acc:0.82\n",
      "epoch 61/100 loss: 1.611867995166002e-06 val_acc:0.82\n",
      "epoch 62/100 loss: 1.6118679951656295e-06 val_acc:0.82\n",
      "epoch 63/100 loss: 1.611867995165374e-06 val_acc:0.82\n",
      "epoch 64/100 loss: 1.6118679951651986e-06 val_acc:0.82\n",
      "epoch 65/100 loss: 1.6118679951650787e-06 val_acc:0.82\n",
      "epoch 66/100 loss: 1.6118679951649953e-06 val_acc:0.82\n",
      "epoch 67/100 loss: 1.6118679951649383e-06 val_acc:0.82\n",
      "epoch 68/100 loss: 1.6118679951648996e-06 val_acc:0.82\n",
      "epoch 69/100 loss: 1.6118679951648729e-06 val_acc:0.82\n",
      "epoch 70/100 loss: 1.6118679951648549e-06 val_acc:0.82\n",
      "epoch 71/100 loss: 1.6118679951648417e-06 val_acc:0.82\n",
      "epoch 72/100 loss: 1.6118679951648326e-06 val_acc:0.82\n",
      "epoch 73/100 loss: 1.6118679951648267e-06 val_acc:0.82\n",
      "epoch 74/100 loss: 1.6118679951648227e-06 val_acc:0.82\n",
      "epoch 75/100 loss: 1.6118679951648193e-06 val_acc:0.82\n",
      "epoch 76/100 loss: 1.6118679951648174e-06 val_acc:0.82\n",
      "epoch 77/100 loss: 1.6118679951648165e-06 val_acc:0.82\n",
      "epoch 78/100 loss: 1.611867995164815e-06 val_acc:0.82\n",
      "epoch 79/100 loss: 1.6118679951648148e-06 val_acc:0.82\n",
      "epoch 80/100 loss: 1.6118679951648142e-06 val_acc:0.82\n",
      "epoch 81/100 loss: 1.6118679951648138e-06 val_acc:0.82\n",
      "epoch 82/100 loss: 1.611867995164814e-06 val_acc:0.82\n",
      "epoch 83/100 loss: 1.6118679951648134e-06 val_acc:0.82\n",
      "epoch 84/100 loss: 1.6118679951648136e-06 val_acc:0.82\n",
      "epoch 85/100 loss: 1.611867995164814e-06 val_acc:0.82\n",
      "epoch 86/100 loss: 1.611867995164813e-06 val_acc:0.82\n",
      "epoch 87/100 loss: 1.6118679951648134e-06 val_acc:0.82\n",
      "epoch 88/100 loss: 1.611867995164813e-06 val_acc:0.82\n",
      "epoch 89/100 loss: 1.6118679951648134e-06 val_acc:0.82\n",
      "epoch 90/100 loss: 1.6118679951648132e-06 val_acc:0.82\n",
      "epoch 91/100 loss: 1.611867995164813e-06 val_acc:0.82\n",
      "epoch 92/100 loss: 1.6118679951648134e-06 val_acc:0.82\n",
      "epoch 93/100 loss: 1.6118679951648132e-06 val_acc:0.82\n",
      "epoch 94/100 loss: 1.6118679951648134e-06 val_acc:0.82\n",
      "epoch 95/100 loss: 1.611867995164813e-06 val_acc:0.82\n",
      "epoch 96/100 loss: 1.6118679951648132e-06 val_acc:0.82\n",
      "epoch 97/100 loss: 1.6118679951648134e-06 val_acc:0.82\n",
      "epoch 98/100 loss: 1.6118679951648134e-06 val_acc:0.82\n",
      "epoch 99/100 loss: 1.6118679951648134e-06 val_acc:0.82\n",
      "At epoch: 99 acc:0.82\n",
      "The accuracy best epoch with val set is 0.8200000000\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(X_train.shape, t_train.shape, X_val.shape, t_val.shape, X_test.shape,\n",
    "      t_test.shape)\n",
    "\n",
    "\n",
    "epoch_best, losses_train, acc_val, acc_best, W_best = train(X_train, t_train, X_val, t_val)\n",
    "\n",
    "\n",
    "print(f\"At epoch: {epoch_best} acc:{acc_best}\")\n",
    "print(f\"The accuracy best epoch with val set is {acc_best:10.10f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The learning curve of the training loss, where x-axis is the number of epochs,\n",
    "and y-axis is the training loss."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"# part a.plot.1\n",
    "fig = plt.figure()\n",
    "plt.plot(losses_train, label=\"Training Losses\")\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()\n",
    "plt.xlabel('Number of epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Doing predictions on the test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "At epoch: 99 acc:0.7843\n",
      "train accuracy:0.82016\n",
      "validation accuracy:0.82\n",
      "test accuracy:0.7843\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "c:\\users\\nathan\\pycharmprojects\\cmput_466_coding_assignment_2\\.venv\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "_, _, _, acc_best, _ = predict(X_test, W_best, t_test)\n",
    "\n",
    "print(f\"At epoch: {epoch_best} acc:{acc_best}\")\n",
    "\n",
    "print(f\"train accuracy:{getAccuracy(X_train, t_train, W_best)}\")\n",
    "print(f\"validation accuracy:{getAccuracy(X_val, t_val, W_best)}\")\n",
    "print(f\"test accuracy:{getAccuracy(X_test, t_test, W_best)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}