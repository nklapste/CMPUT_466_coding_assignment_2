{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CMPUT 466/566, Winter 2020 Introduction to Machine learning \n",
    "## Coding Assignment 2 \n",
    "### Problem 1 Report\n",
    "By Nathan Klapstein #1449872"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "\n",
    "import struct\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.special import expit\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def readMNISTdata():\n",
    "    with open('data/t10k-images-idx3-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "        test_data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        test_data = test_data.reshape((size, nrows * ncols))\n",
    "\n",
    "    with open('data/t10k-labels-idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        test_labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        test_labels = test_labels.reshape((size, 1))\n",
    "\n",
    "    with open('data/train-images-idx3-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "        train_data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        train_data = train_data.reshape((size, nrows * ncols))\n",
    "\n",
    "    with open('data/train-labels-idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        train_labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        train_labels = train_labels.reshape((size, 1))\n",
    "\n",
    "    # augmenting a constant feature of 1 (absorbing the bias term)\n",
    "    train_data = np.concatenate(\n",
    "        (np.ones([train_data.shape[0], 1]), train_data), axis=1)\n",
    "    test_data = np.concatenate((np.ones([test_data.shape[0], 1]), test_data),\n",
    "                               axis=1)\n",
    "    np.random.seed(314)\n",
    "    np.random.shuffle(train_labels)\n",
    "    np.random.seed(314)\n",
    "    np.random.shuffle(train_data)\n",
    "\n",
    "    X_train = train_data[:50000] / 256\n",
    "    t_train = train_labels[:50000]\n",
    "\n",
    "    X_val = train_data[50000:] / 256\n",
    "    t_val = train_labels[50000:]\n",
    "\n",
    "    return X_train, t_train, X_val, t_val, test_data, test_labels\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load in the MNIST data for later compute."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    " X_train, t_train, X_val, t_val, X_test, t_test = readMNISTdata()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gradient decent computation.\n",
    "\n",
    "Recycled and fixed from CMPUT 466 Coding Assignment 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def gradient(X, w, y=None):\n",
    "    # TODO: FIXED:  Did not scale gradient by 1/M;\n",
    "    # TODO: CHECK: 1/M  this is 1/N_train or should be 1/X.shape[0]=1/batch_size\n",
    "    return (1/X.shape[0])*np.matmul(np.transpose(X), (np.matmul(X, w) - y))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Various global configurations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "N_class = 10\n",
    "alpha = 0.4  # learning rate\n",
    "batch_size = 1000000  # batch size\n",
    "MaxIter = 50  # Maximum iteration\n",
    "decay = 0.  # weight decay"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mini-batch linear gradient decent.\n",
    "\n",
    "Recycled and fixed from CMPUT 466 Coding assignment 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# def computeLoss(self, x, yMatrix):\n",
    "#    \"\"\"\n",
    "#    It calculates a cross-entropy loss with regularization loss and gradient to update the weights.\n",
    "#    :param x: An input sample\n",
    "#    :param yMatrix: Label as one-hot encoding\n",
    "#    :return:\n",
    "#    \"\"\"\n",
    "#    numOfSamples = x.shape[0]\n",
    "#    scores = np.dot(x, self.wt)\n",
    "#    prob = self.softmaxEquation(scores)\n",
    "# \n",
    "#    loss = -np.log(np.max(prob)) * yMatrix\n",
    "#    regLoss = (1/2)*self.regStrength*np.sum(self.wt*self.wt)\n",
    "#    totalLoss = (np.sum(loss) / numOfSamples) + regLoss\n",
    "#    grad = ((-1 / numOfSamples) * np.dot(x.T, (yMatrix - prob))) + (self.regStrength * self.wt)\n",
    "#    return totalLoss, grad\n",
    "\n",
    "def softmaxEquation(scores):\n",
    "    \"\"\"\n",
    "    It calculates a softmax probability\n",
    "    :param scores: A matrix(wt * input sample)\n",
    "    :return: softmax probability\n",
    "    \"\"\"\n",
    "    scores -= np.max(scores)\n",
    "    prob = (np.exp(scores).T / np.sum(np.exp(scores), axis=1)).T\n",
    "    return prob\n",
    "\n",
    "# \n",
    "# def meanAccuracy(self, x, y):\n",
    "#     \"\"\"\n",
    "#     It calculates mean-per class accuracy\n",
    "#     :param x: Input sample\n",
    "#     :param y: label sample\n",
    "#     :return: mean-per class accuracy\n",
    "#     \"\"\"\n",
    "#     predY = predict(x)\n",
    "#     predY = predY.reshape((-1, 1))  # convert to column vector\n",
    "#     return np.mean(np.equal(y, predY))\n",
    "# \n",
    "\n",
    "\n",
    "def oneHotIt(Y):\n",
    "    m = Y.shape[0]\n",
    "    Y = Y[:,0]\n",
    "    OHX = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n",
    "    OHX = np.array(OHX.todense()).T\n",
    "    return OHX\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n",
    "    return sm\n",
    "\n",
    "\n",
    "def getLoss(w,x,y,lam):\n",
    "    m = x.shape[0] #First we get the number of training examples\n",
    "    y_mat = oneHotIt(y) #Next we convert the integer class coding into a one-hot representation\n",
    "    scores = np.dot(x,w) #Then we compute raw class scores given our input and current weights\n",
    "    prob = softmax(scores) #Next we perform a softmax on these scores to get their probabilities\n",
    "    loss = (-1 / m) * np.sum(y_mat * np.log(prob)) + (lam/2)*np.sum(w*w) #We then find the loss of the probabilities\n",
    "    # print(x.shape)\n",
    "    # print(y.shape)\n",
    "    # print(y_mat.shape)\n",
    "    # print(y_mat)\n",
    "    # print(prob.shape)\n",
    "    print(y_mat[0])\n",
    "    print(prob[0])\n",
    "    print(np.sum(prob[0]))\n",
    "    # print(w.shape)\n",
    "    grad = (-1 / m) * np.dot(x.T, (y_mat - prob)) + lam*w #And compute the gradient for that loss\n",
    "    # print(grad.shape)\n",
    "    return loss, grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getProbsAndPreds(someX, w):\n",
    "    probs = softmax(np.dot(someX, w))\n",
    "    preds = np.argmax(probs,axis=1)\n",
    "    return probs,preds\n",
    "\n",
    "\n",
    "# inspired by https://medium.com/@awjuliani/simple-softmax-in-python-tutorial-d6b4c4ed5c16\n",
    "# https://www.w3resource.com/numpy/manipulation/ndarray-flatten.php\n",
    "def getAccuracy(someX,someY,w):\n",
    "    prob,prede = getProbsAndPreds(someX,w)\n",
    "    someY = someY.flatten()\n",
    "    # print(prede, someY)\n",
    "    # print(prede.shape, someY.shape)\n",
    "    num_correct = np.sum(prede == someY)\n",
    "    num_incorrect = np.sum(prede != someY)\n",
    "    # print(num_correct)\n",
    "    # print(num_incorrect)\n",
    "    accuracy = num_correct/(num_correct+num_incorrect)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def predict(X, W, t=None):\n",
    "    # X_new: Nsample x (d+1)\n",
    "    # W: (d+1) x K\n",
    "    \n",
    "    reg = 0.001\n",
    "    # TODO Your code here\n",
    "    y = np.matmul(X, W)\n",
    "    \n",
    "    # y = np.argmax(X.dot(W), 1)\n",
    "    t_hat = np.argmax(y, axis=1)\n",
    "    num_of_samples = X.shape[0]\n",
    "\n",
    "    z = np.dot(X, W)\n",
    "    z -= np.max(z, axis=0)  # Max trick for the softmax, preventing infinite values\n",
    "    y_prob = np.exp(z) / np.sum(np.exp(z)) \n",
    "    # ~z_i = z_i - z_max_i\n",
    "    # exp_z = np.exp(z - np.max(z, axis=0) + np.min(z, axis=0))\n",
    "    # p = exp_z / np.sum(exp_z, axis=0) # softmax \n",
    "    # p = softmaxEquation(z)\n",
    "     \n",
    "    # compute the loss\n",
    "    loss = -np.log(np.max(y_prob)) * t\n",
    "    reg_loss = 0.5  * reg * np.sum(np.multiply(W, W))  # Regularization term\n",
    "    cross_entropy_loss =  (1 / num_of_samples) * (np.sum(loss)) \n",
    "    total_loss = cross_entropy_loss + reg_loss\n",
    "    # total_loss = cross_entropy_loss\n",
    "    \n",
    "    # compute the mean-per class accuracy\n",
    "    acc = getAccuracy(X, t, W)\n",
    "    \n",
    "    # Calculation of dW (gradient)\n",
    "    grad = ((-1 / num_of_samples) * np.dot(X.T, (t - y_prob))) + (reg * W)\n",
    "    \n",
    "    \n",
    "    return y, t_hat, total_loss, acc, grad\n",
    "\n",
    "\n",
    "\n",
    "def train(X_train, t_train, X_val, t_val):\n",
    "    N_train = X_train.shape[0]\n",
    "    N_val = X_val.shape[0]\n",
    "\n",
    "    # TODO Your code here\n",
    "    \n",
    "    # initialization\n",
    "    w = np.zeros([X_train.shape[1], N_class])\n",
    "    # w: (d+1)x1\n",
    "\n",
    "    losses_train = []\n",
    "    acc_val = []\n",
    "    w_best = None\n",
    "    acc_best = 0\n",
    "    epoch_best = 0\n",
    "    \n",
    "    # TODO: figure this shit out\n",
    "    lam = 0.4\n",
    "\n",
    "\n",
    "    for epoch in range(MaxIter):\n",
    "        loss_this_epoch = 0\n",
    "        for b in range(int(np.ceil(N_train / batch_size))):\n",
    "            X_batch = X_train[b * batch_size: (b + 1) * batch_size]\n",
    "            t_batch = t_train[b * batch_size: (b + 1) * batch_size]\n",
    "            loss_batch, grad = getLoss(w, X_batch, t_batch, lam)\n",
    "            loss_this_epoch += loss_batch\n",
    "            # TODO: grad is wrong shape (785,10) when it should be (785,1)\n",
    "            if b ==5:\n",
    "                return\n",
    "            w = w - (alpha * grad)\n",
    "            # y_hat_batch, t_hat, loss_batch, acc, grad = predict(X_batch, w, t_batch)\n",
    "            # # print(loss_batch)\n",
    "            # loss_this_epoch += loss_batch\n",
    "            # # TODO: Your code here\n",
    "            # # Mini-batch gradient descent\n",
    "            # #TODO: FIX: scale gradient with 1/M\n",
    "            # # w = w - alpha * gradient(X_batch, w, y_hat_batch)\n",
    "            # w -= alpha * grad\n",
    "   \n",
    "        # TODO: Your code here\n",
    "        # monitor model behavior after each epoch\n",
    "        \n",
    "        # 1. Compute the training loss by averaging loss_this_epoch\n",
    "        #         losses_train.append(loss_this_epoch/int(np.ceil(N_train/batch_size)))\n",
    "        \n",
    "    \n",
    "        loss_this_epoch = loss_this_epoch / (int(np.ceil(batch_size)))\n",
    "        val_acc = getAccuracy(X_val, t_val, w)\n",
    "        print(f\"epoch {epoch}/{MaxIter} loss: {loss_this_epoch} val_acc:{val_acc}\")\n",
    "        \n",
    "        # test_acc = getAccuracy(X_train, t_train, w)\n",
    "        # print(f\"est_acc:{test_acc} val_acc:{val_acc}\")\n",
    "        losses_train.append(loss_this_epoch)\n",
    "        #print(f\"epoch={epoch} training loss={loss_this_epoch/  (int(np.ceil(N_train/batch_size)))}\")\n",
    "\n",
    "        # 2. Perform validation on the validation test by the risk\n",
    "        # risk_this_epoch = 0\n",
    "        # for c in range(int(np.ceil(N_val / batch_size))):\n",
    "        #     X_val_batch = X_val[c * batch_size: (c + 1) * batch_size]\n",
    "        #     y_val_batch = y_val[c * batch_size: (c + 1) * batch_size]\n",
    "        #     _, _, risk = predict(X_val_batch, w, y_val_batch)\n",
    "        #     risk_this_epoch += risk\n",
    "        # TODO: FIXED: Validation set should not be run in batches\n",
    "        # y, t_hat, _, acc, grad = predict(X_val, w, t_val)\n",
    "        acc_val.append(val_acc)\n",
    "        \n",
    "        # 3. Keep track of the best validation epoch, risk, and the weights\n",
    "        if acc_val[epoch] >= acc_best:\n",
    "            epoch_best = epoch\n",
    "            acc_best = acc_val[epoch]\n",
    "            w_best = w\n",
    "\n",
    "    # Return some variables as needed\n",
    "    return epoch_best, losses_train, acc_val, acc_best, w_best\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(50000, 785) (50000, 1) (10000, 785) (10000, 1) (10000, 785) (10000, 1)\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "1.0\n",
      "epoch 0/50 loss: 2.3025850929940443e-06 val_acc:0.665\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.11149817 0.06076857 0.0681313  0.14819635 0.07981978 0.09037112\n",
      " 0.08194523 0.11309085 0.12941517 0.11676347]\n",
      "1.0\n",
      "epoch 1/50 loss: 1.951664044804229e-06 val_acc:0.7719\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.06877572 0.07193152 0.04621658 0.15421728 0.08600277 0.12002405\n",
      " 0.06917613 0.14715406 0.10183931 0.13466258]\n",
      "1.0\n",
      "epoch 2/50 loss: 1.7791795022920606e-06 val_acc:0.7714\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.07273218 0.06188347 0.03523528 0.17629867 0.07485062 0.12052151\n",
      " 0.05802757 0.14639006 0.1170793  0.13698133]\n",
      "0.9999999999999999\n",
      "epoch 3/50 loss: 1.6959260212888437e-06 val_acc:0.7928\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.05489202 0.05880338 0.0294507  0.18196919 0.07484334 0.13772169\n",
      " 0.05423133 0.16047063 0.10056577 0.14705194]\n",
      "1.0\n",
      "epoch 4/50 loss: 1.6551098477575747e-06 val_acc:0.7947\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.05889816 0.05510292 0.02491815 0.18809744 0.07001408 0.13881438\n",
      " 0.04852599 0.15939397 0.10868062 0.14755428]\n",
      "0.9999999999999999\n",
      "epoch 5/50 loss: 1.6346090434098395e-06 val_acc:0.8026\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.05102841 0.05274617 0.02301076 0.1949793  0.06854693 0.14836794\n",
      " 0.04734275 0.16387508 0.09828713 0.15181553]\n",
      "0.9999999999999999\n",
      "epoch 6/50 loss: 1.624113141115669e-06 val_acc:0.8051\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.05346027 0.05124918 0.02094623 0.19499887 0.06651253 0.14878326\n",
      " 0.04462334 0.1637889  0.10332032 0.15231709]\n",
      "1.0\n",
      "epoch 7/50 loss: 1.6186032515847664e-06 val_acc:0.8087\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04986723 0.05001853 0.02018785 0.20100282 0.06524218 0.15388862\n",
      " 0.04415171 0.16509797 0.09648819 0.1540549 ]\n",
      "1.0\n",
      "epoch 8/50 loss: 1.6156542514547504e-06 val_acc:0.8104\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.05117737 0.04938712 0.01920035 0.19948323 0.06429375 0.15407608\n",
      " 0.04291406 0.16516863 0.09994847 0.15435094]\n",
      "0.9999999999999999\n",
      "epoch 9/50 loss: 1.6140390914197145e-06 val_acc:0.8125\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04950254 0.04879275 0.01886098 0.20389321 0.06347251 0.15670962\n",
      " 0.04268743 0.16554561 0.09536628 0.15516907]\n",
      "1.0\n",
      "epoch 10/50 loss: 1.613137331664936e-06 val_acc:0.8135\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.05018014 0.04852263 0.01837165 0.20232145 0.06297591 0.15683684\n",
      " 0.04213883 0.16557693 0.09782543 0.1552502 ]\n",
      "0.9999999999999998\n",
      "epoch 11/50 loss: 1.6126230537501062e-06 val_acc:0.8145\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.0493879  0.04824712 0.01821283 0.20533918 0.06249279 0.15817801\n",
      " 0.04202809 0.16566612 0.09473534 0.15571261]\n",
      "0.9999999999999999\n",
      "epoch 12/50 loss: 1.6123244823052373e-06 val_acc:0.8157\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04973122 0.04813236 0.01796285 0.20407011 0.06221335 0.15826833\n",
      " 0.04179032 0.1656681  0.09648961 0.15567374]\n",
      "1.0\n",
      "epoch 13/50 loss: 1.6121478547590488e-06 val_acc:0.8161\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04935144 0.04800993 0.01788893 0.20607897 0.06193754 0.15895373\n",
      " 0.04174047 0.1656657  0.09440331 0.15596998]\n",
      "0.9999999999999999\n",
      "epoch 14/50 loss: 1.612041766496415e-06 val_acc:0.8168\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04952325 0.04796279 0.0177577  0.205134   0.06177493 0.15901153\n",
      " 0.04164123 0.16565836 0.09564985 0.15588636]\n",
      "1.0000000000000002\n",
      "epoch 15/50 loss: 1.611977047678842e-06 val_acc:0.8174\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.0493384  0.04791165 0.0177248  0.20645888 0.06161822 0.15936595\n",
      " 0.04162231 0.16563129 0.09424046 0.15608805]\n",
      "1.0\n",
      "epoch 16/50 loss: 1.6119370897967837e-06 val_acc:0.818\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04942373 0.0478938  0.01765416 0.2057791  0.0615222  0.15939852\n",
      " 0.04158402 0.16562332 0.09512172 0.15599945]\n",
      "1.0\n",
      "epoch 17/50 loss: 1.6119121177245892e-06 val_acc:0.8181\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04933207 0.0478748  0.01764092 0.20665182 0.06143266 0.15958464\n",
      " 0.04157954 0.16559472 0.09416889 0.15613996]\n",
      "1.0\n",
      "epoch 18/50 loss: 1.611896370279385e-06 val_acc:0.8186\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04937427 0.04786932 0.01760197 0.20617095 0.06137551 0.1596003\n",
      " 0.04156733 0.16558863 0.09478899 0.15606272]\n",
      "1.0\n",
      "epoch 19/50 loss: 1.6118863483953785e-06 val_acc:0.8187\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04932778 0.04786414 0.01759782 0.20674725 0.06132385 0.15969969\n",
      " 0.04156878 0.16556534 0.09414419 0.15616115]\n",
      "1.0\n",
      "epoch 20/50 loss: 1.6118799282271111e-06 val_acc:0.8192\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04934864 0.04786359 0.01757582 0.20641013 0.06128963 0.15970533\n",
      " 0.04156709 0.16556135 0.09457877 0.15609965]\n",
      "1.0000000000000002\n",
      "epoch 21/50 loss: 1.6118757871646076e-06 val_acc:0.8193\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04932441 0.04786388 0.01757561 0.20679221 0.06125948 0.15975932\n",
      " 0.04157041 0.16554424 0.09414193 0.15616851]\n",
      "1.0\n",
      "epoch 22/50 loss: 1.611873103211103e-06 val_acc:0.8193\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04933475 0.04786504 0.01756288 0.20655709 0.06123884 0.15975967\n",
      " 0.04157243 0.1655419  0.09444546 0.15612194]\n",
      "0.9999999999999999\n",
      "epoch 23/50 loss: 1.6118713547165584e-06 val_acc:0.8193\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04932173 0.04786713 0.01756414 0.20681154 0.06122103 0.1597895\n",
      " 0.04157586 0.16552998 0.09414917 0.15616993]\n",
      "1.0\n",
      "epoch 24/50 loss: 1.6118702115001776e-06 val_acc:0.8197\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04932688 0.04786867 0.01755661 0.20664809 0.06120848 0.15978747\n",
      " 0.04157875 0.16552875 0.09436056 0.15613572]\n",
      "0.9999999999999998\n",
      "epoch 25/50 loss: 1.6118694610968176e-06 val_acc:0.8195\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931966 0.04787104 0.01755822 0.20681828 0.06119783 0.15980424\n",
      " 0.04158166 0.1655207  0.09415936 0.15616902]\n",
      "1.0\n",
      "epoch 26/50 loss: 1.6118689671452134e-06 val_acc:0.8198\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04932225 0.04787246 0.01755366 0.20670491 0.06119014 0.15980145\n",
      " 0.04158435 0.16552016 0.09430623 0.15614438]\n",
      "1.0\n",
      "epoch 27/50 loss: 1.611868641000408e-06 val_acc:0.8197\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931811 0.04787455 0.01755516 0.20681921 0.06118368 0.15981103\n",
      " 0.04158659 0.16551481 0.09416946 0.1561674 ]\n",
      "1.0\n",
      "epoch 28/50 loss: 1.6118684251646362e-06 val_acc:0.8199\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931943 0.04787571 0.01755235 0.2067407  0.06117892 0.15980828\n",
      " 0.04158876 0.16551466 0.0942713  0.15614988]\n",
      "1.0000000000000002\n",
      "epoch 29/50 loss: 1.611868281973504e-06 val_acc:0.8199\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931698 0.04787737 0.01755359 0.20681773 0.06117496 0.15981386\n",
      " 0.04159041 0.16551115 0.09417822 0.15616574]\n",
      "1.0000000000000002\n",
      "epoch 30/50 loss: 1.6118681867961127e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931767 0.04787826 0.01755182 0.20676344 0.06117198 0.15981149\n",
      " 0.04159203 0.1655112  0.09424872 0.15615338]\n",
      "1.0000000000000002\n",
      "epoch 31/50 loss: 1.6118681234021232e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931617 0.04787951 0.01755279 0.20681551 0.06116952 0.15981479\n",
      " 0.04159321 0.1655089  0.09418531 0.15616428]\n",
      "1.0\n",
      "epoch 32/50 loss: 1.611868081108695e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931654 0.04788017 0.01755166 0.20677801 0.06116764 0.15981289\n",
      " 0.04159438 0.16550904 0.09423404 0.15615562]\n",
      "0.9999999999999999\n",
      "epoch 33/50 loss: 1.6118680528429351e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.0493156  0.04788109 0.01755238 0.2068133  0.06116609 0.15981488\n",
      " 0.04159519 0.16550755 0.09419082 0.15616309]\n",
      "1.0000000000000002\n",
      "epoch 34/50 loss: 1.6118680339249542e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931581 0.04788156 0.01755165 0.20678743 0.0611649  0.15981342\n",
      " 0.04159602 0.16550771 0.09422446 0.15615705]\n",
      "1.0000000000000002\n",
      "epoch 35/50 loss: 1.6118680212440066e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931521 0.04788222 0.01755218 0.2068114  0.06116391 0.15981465\n",
      " 0.04159658 0.16550673 0.09419497 0.15616217]\n",
      "0.9999999999999998\n",
      "epoch 36/50 loss: 1.611868012732812e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931532 0.04788256 0.0175517  0.20679355 0.06116314 0.15981356\n",
      " 0.04159715 0.16550688 0.09421817 0.15615796]\n",
      "1.0\n",
      "epoch 37/50 loss: 1.6118680070125624e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931493 0.04788302 0.01755208 0.20680986 0.06116251 0.15981433\n",
      " 0.04159753 0.16550625 0.09419803 0.15616146]\n",
      "1.0\n",
      "epoch 38/50 loss: 1.611868003163544e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.049315   0.04788326 0.01755176 0.20679757 0.06116201 0.15981355\n",
      " 0.04159792 0.16550638 0.09421402 0.15615854]\n",
      "1.0\n",
      "epoch 39/50 loss: 1.6118680005705018e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931474 0.04788358 0.01755203 0.20680869 0.0611616  0.15981404\n",
      " 0.04159817 0.16550597 0.09420027 0.15616092]\n",
      "1.0\n",
      "epoch 40/50 loss: 1.6118679988217125e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931478 0.04788375 0.01755182 0.20680022 0.06116127 0.15981348\n",
      " 0.04159843 0.16550607 0.09421128 0.1561589 ]\n",
      "1.0\n",
      "epoch 41/50 loss: 1.6118679976410096e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931461 0.04788398 0.01755201 0.20680781 0.061161   0.1598138\n",
      " 0.0415986  0.1655058  0.09420187 0.15616053]\n",
      "0.9999999999999998\n",
      "epoch 42/50 loss: 1.6118679968430594e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931464 0.04788409 0.01755186 0.20680199 0.06116078 0.1598134\n",
      " 0.04159878 0.16550588 0.09420945 0.15615913]\n",
      "0.9999999999999999\n",
      "epoch 43/50 loss: 1.6118679963032436e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931452 0.04788425 0.017552   0.20680717 0.06116061 0.15981361\n",
      " 0.04159889 0.1655057  0.09420302 0.15616024]\n",
      "1.0\n",
      "epoch 44/50 loss: 1.6118679959377189e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931454 0.04788432 0.0175519  0.20680316 0.06116046 0.15981333\n",
      " 0.04159901 0.16550577 0.09420823 0.15615927]\n",
      "0.9999999999999999\n",
      "epoch 45/50 loss: 1.6118679956899848e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931446 0.04788443 0.01755199 0.2068067  0.06116035 0.15981348\n",
      " 0.04159909 0.16550565 0.09420383 0.15616002]\n",
      "1.0\n",
      "epoch 46/50 loss: 1.611867995521937e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931447 0.04788449 0.01755193 0.20680395 0.06116025 0.15981328\n",
      " 0.04159917 0.1655057  0.09420741 0.15615935]\n",
      "1.0\n",
      "epoch 47/50 loss: 1.6118679954078474e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931441 0.04788456 0.01755199 0.20680638 0.06116017 0.15981338\n",
      " 0.04159922 0.16550562 0.0942044  0.15615987]\n",
      "1.0\n",
      "epoch 48/50 loss: 1.611867995330328e-06 val_acc:0.82\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.04931442 0.0478846  0.01755195 0.20680448 0.06116011 0.15981325\n",
      " 0.04159927 0.16550565 0.09420686 0.15615941]\n",
      "0.9999999999999999\n",
      "epoch 49/50 loss: 1.6118679952776153e-06 val_acc:0.82\n",
      "At epoch 49 val:  0.82\n",
      "The accuracy best epoch with val set is 0.8200000000\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(X_train.shape, t_train.shape, X_val.shape, t_val.shape, X_test.shape,\n",
    "      t_test.shape)\n",
    "\n",
    "\n",
    "epoch_best, losses_train, acc_val, acc_best, W_best = train(X_train, t_train, X_val, t_val)\n",
    "\n",
    "\n",
    "print('At epoch', epoch_best, 'val: ', acc_best,)\n",
    "print(f\"The accuracy best epoch with val set is {acc_best:10.10f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The learning curve of the training loss, where x-axis is the number of epochs,\n",
    "and y-axis is the training loss."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"# part a.plot.1\n",
    "fig = plt.figure()\n",
    "plt.plot(losses_train, label=\"Training Losses\")\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()\n",
    "plt.xlabel('Number of epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Doing predictions on the test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "At epoch 49 val:  0.82 test: 0.7843\n",
      "The loss of best epoch with test data is  10.5465372016\n",
      "The accuracy best epoch with test set is 0.7843000000\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "c:\\users\\nathan\\pycharmprojects\\cmput_466_coding_assignment_2\\.venv\\lib\\site-packages\\ipykernel_launcher.py:52: RuntimeWarning: invalid value encountered in true_divide\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "y_test, t_hat_test, loss_test, acc_test, grad = predict(X_test, W_best, t_test)\n",
    "\n",
    "print('At epoch', epoch_best, 'val: ', acc_best, 'test:', acc_test)\n",
    "print(f\"The loss of best epoch with test data is  {loss_test:10.10f}\")\n",
    "print(f\"The accuracy best epoch with test set is {acc_test:10.10f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "train accuracy:0.82016\n",
      "validation accuracy:0.82\n",
      "test accuracy:0.7843\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "c:\\users\\nathan\\pycharmprojects\\cmput_466_coding_assignment_2\\.venv\\lib\\site-packages\\ipykernel_launcher.py:52: RuntimeWarning: invalid value encountered in true_divide\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "print(f\"train accuracy:{getAccuracy(X_train, t_train, W_best)}\")\n",
    "print(f\"validation accuracy:{getAccuracy(X_val, t_val, W_best)}\")\n",
    "print(f\"test accuracy:{getAccuracy(X_test, t_test, W_best)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}