{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CMPUT 466/566, Winter 2020 Introduction to Machine learning \n",
    "## Coding Assignment 2 \n",
    "### Problem 1 Report\n",
    "By Nathan Klapstein #1449872"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "import struct\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.special import expit\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def readMNISTdata():\n",
    "    with open('data/t10k-images-idx3-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "        test_data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        test_data = test_data.reshape((size, nrows * ncols))\n",
    "\n",
    "    with open('data/t10k-labels-idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        test_labels = np.fromfile(f,\n",
    "                                  dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        test_labels = test_labels.reshape((size, 1))\n",
    "\n",
    "    with open('data/train-images-idx3-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "        train_data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        train_data = train_data.reshape((size, nrows * ncols))\n",
    "\n",
    "    with open('data/train-labels-idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        train_labels = np.fromfile(f,\n",
    "                                   dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        train_labels = train_labels.reshape((size, 1))\n",
    "\n",
    "    # augmenting a constant feature of 1 (absorbing the bias term)\n",
    "    train_data = np.concatenate(\n",
    "        (np.ones([train_data.shape[0], 1]), train_data), axis=1)\n",
    "    test_data = np.concatenate((np.ones([test_data.shape[0], 1]), test_data),\n",
    "                               axis=1)\n",
    "    np.random.seed(314)\n",
    "    np.random.shuffle(train_labels)\n",
    "    np.random.seed(314)\n",
    "    np.random.shuffle(train_data)\n",
    "\n",
    "    X_train = train_data[:50000] / 256\n",
    "    t_train = train_labels[:50000]\n",
    "\n",
    "    X_val = train_data[50000:] / 256\n",
    "    t_val = train_labels[50000:]\n",
    "\n",
    "    return X_train, t_train, X_val, t_val, test_data, test_labels\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load in the MNIST data for later compute."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    " X_train, t_train, X_val, t_val, X_test, t_test = readMNISTdata()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gradient decent computation.\n",
    "\n",
    "Recycled and fixed from CMPUT 466 Coding Assignment 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def gradient(X, w, y=None):\n",
    "    # TODO: FIXED:  Did not scale gradient by 1/M;\n",
    "    # TODO: CHECK: 1/M  this is 1/N_train or should be 1/X.shape[0]=1/batch_size\n",
    "    return (1/X.shape[0])*np.matmul(np.transpose(X), (np.matmul(X, w) - y))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Various global configurations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "N_class = 10\n",
    "alpha = 0.1  # learning rate\n",
    "batch_size = 100  # batch size\n",
    "MaxIter = 50  # Maximum iteration\n",
    "decay = 0.  # weight decay"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# def computeLoss(self, x, yMatrix):\n",
    "#    \"\"\"\n",
    "#    It calculates a cross-entropy loss with regularization loss and gradient to update the weights.\n",
    "#    :param x: An input sample\n",
    "#    :param yMatrix: Label as one-hot encoding\n",
    "#    :return:\n",
    "#    \"\"\"\n",
    "#    numOfSamples = x.shape[0]\n",
    "#    scores = np.dot(x, self.wt)\n",
    "#    prob = self.softmaxEquation(scores)\n",
    "# \n",
    "#    loss = -np.log(np.max(prob)) * yMatrix\n",
    "#    regLoss = (1/2)*self.regStrength*np.sum(self.wt*self.wt)\n",
    "#    totalLoss = (np.sum(loss) / numOfSamples) + regLoss\n",
    "#    grad = ((-1 / numOfSamples) * np.dot(x.T, (yMatrix - prob))) + (self.regStrength * self.wt)\n",
    "#    return totalLoss, grad\n",
    "\n",
    "def softmaxEquation(scores):\n",
    "    \"\"\"\n",
    "    It calculates a softmax probability\n",
    "    :param scores: A matrix(wt * input sample)\n",
    "    :return: softmax probability\n",
    "    \"\"\"\n",
    "    scores -= np.max(scores)\n",
    "    prob = (np.exp(scores).T / np.sum(np.exp(scores), axis=1)).T\n",
    "    return prob\n",
    "\n",
    "# \n",
    "# def meanAccuracy(self, x, y):\n",
    "#     \"\"\"\n",
    "#     It calculates mean-per class accuracy\n",
    "#     :param x: Input sample\n",
    "#     :param y: label sample\n",
    "#     :return: mean-per class accuracy\n",
    "#     \"\"\"\n",
    "#     predY = predict(x)\n",
    "#     predY = predY.reshape((-1, 1))  # convert to column vector\n",
    "#     return np.mean(np.equal(y, predY))\n",
    "# \n",
    "\n",
    "\n",
    "def oneHotIt(Y):\n",
    "    m = Y.shape[0]\n",
    "    Y = Y[:,0]\n",
    "    OHX = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n",
    "    OHX = np.array(OHX.todense()).T\n",
    "    return OHX\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n",
    "    return sm\n",
    "\n",
    "\n",
    "def getLoss(w,x,y,lam):\n",
    "    m = x.shape[0] #First we get the number of training examples\n",
    "    y_mat = oneHotIt(y) #Next we convert the integer class coding into a one-hot representation\n",
    "    scores = np.dot(x,w) #Then we compute raw class scores given our input and current weights\n",
    "    prob = softmax(scores) #Next we perform a softmax on these scores to get their probabilities\n",
    "    loss = (-1 / m) * np.sum(y_mat * np.log(prob)) + (lam/2)*np.sum(w*w) #We then find the loss of the probabilities\n",
    "    grad = (-1 / m) * np.dot(x.T, (y_mat - prob)) + lam*w #And compute the gradient for that loss\n",
    "    return loss,grad\n",
    "\n",
    "def predict(X, W, t=None):\n",
    "    # X_new: Nsample x (d+1)\n",
    "    # W: (d+1) x K\n",
    "    \n",
    "    reg = 0.001\n",
    "    # TODO Your code here\n",
    "    y = np.matmul(X, W)\n",
    "    \n",
    "    # y = np.argmax(X.dot(W), 1)\n",
    "    t_hat = np.argmax(y, axis=1)\n",
    "    num_of_samples = X.shape[0]\n",
    "\n",
    "    z = np.dot(X, W)\n",
    "    z -= np.max(z, axis=0)  # Max trick for the softmax, preventing infinite values\n",
    "    y_prob = np.exp(z) / np.sum(np.exp(z)) \n",
    "    # ~z_i = z_i - z_max_i\n",
    "    # exp_z = np.exp(z - np.max(z, axis=0) + np.min(z, axis=0))\n",
    "    # p = exp_z / np.sum(exp_z, axis=0) # softmax \n",
    "    # p = softmaxEquation(z)\n",
    "     \n",
    "    # compute the loss\n",
    "    loss = -np.log(np.max(y_prob)) * t\n",
    "    reg_loss = 0.5  * reg * np.sum(np.multiply(W, W))  # Regularization term\n",
    "    cross_entropy_loss =  (1 / num_of_samples) * (np.sum(loss)) \n",
    "    total_loss = cross_entropy_loss + reg_loss\n",
    "    # total_loss = cross_entropy_loss\n",
    "    \n",
    "    # compute the mean-per class accuracy\n",
    "    acc = np.mean(np.equal(t, t_hat))\n",
    "    \n",
    "    # Calculation of dW (gradient)\n",
    "    grad = ((-1 / num_of_samples) * np.dot(X.T, (t - y_prob))) + (reg * W)\n",
    "    \n",
    "    \n",
    "    return y, t_hat, total_loss, acc, grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mini-batch linear gradient decent.\n",
    "\n",
    "Recycled and fixed from CMPUT 466 Coding assignment 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "def getProbsAndPreds(someX, w):\n",
    "    probs = softmax(np.dot(someX, w))\n",
    "    preds = np.argmax(probs,axis=1)\n",
    "    return probs,preds\n",
    "\n",
    "def getAccuracy(someX,someY,w):\n",
    "    prob,prede = getProbsAndPreds(someX,w)\n",
    "    accuracy = np.sum(prede == someY)/(float(len(someY)))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train(X_train, t_train, X_val, t_val):\n",
    "    N_train = X_train.shape[0]\n",
    "    N_val = X_val.shape[0]\n",
    "\n",
    "    # TODO Your code here\n",
    "    \n",
    "    # initialization\n",
    "    w = np.zeros([X_train.shape[1], 1])\n",
    "    # w: (d+1)x1\n",
    "\n",
    "    losses_train = []\n",
    "    acc_val = []\n",
    "    w_best = None\n",
    "    acc_best = 0\n",
    "    epoch_best = 0\n",
    "    \n",
    "    # TODO: figure this shit out\n",
    "    lam = .01\n",
    "\n",
    "\n",
    "    for epoch in range(MaxIter):\n",
    "        loss_this_epoch = 0\n",
    "        for b in range(int(np.ceil(N_train / batch_size))):\n",
    "            X_batch = X_train[b * batch_size: (b + 1) * batch_size]\n",
    "            t_batch = t_train[b * batch_size: (b + 1) * batch_size]\n",
    "            loss_batch, grad = getLoss(w, X_batch, t_batch, lam)\n",
    "            loss_this_epoch+= loss_batch\n",
    "            \n",
    "            w = w - (alpha * grad)\n",
    "            # y_hat_batch, t_hat, loss_batch, acc, grad = predict(X_batch, w, t_batch)\n",
    "            # # print(loss_batch)\n",
    "            # loss_this_epoch += loss_batch\n",
    "            # # TODO: Your code here\n",
    "            # # Mini-batch gradient descent\n",
    "            # #TODO: FIX: scale gradient with 1/M\n",
    "            # # w = w - alpha * gradient(X_batch, w, y_hat_batch)\n",
    "            # w -= alpha * grad\n",
    "   \n",
    "        # TODO: Your code here\n",
    "        # monitor model behavior after each epoch\n",
    "        \n",
    "        # 1. Compute the training loss by averaging loss_this_epoch\n",
    "        #         losses_train.append(loss_this_epoch/int(np.ceil(N_train/batch_size)))\n",
    "        \n",
    "    \n",
    "        loss_this_epoch = loss_this_epoch / (int(np.ceil(batch_size)))\n",
    "        \n",
    "        print(f\"epoch {epoch}/{MaxIter} loss: {loss_this_epoch}\")\n",
    "        \n",
    "        # test_acc = getAccuracy(X_train, t_train, w)\n",
    "        val_acc = getAccuracy(X_val, t_val, w)\n",
    "        # print(f\"est_acc:{test_acc} val_acc:{val_acc}\")\n",
    "        losses_train.append(loss_this_epoch)\n",
    "        #print(f\"epoch={epoch} training loss={loss_this_epoch/  (int(np.ceil(N_train/batch_size)))}\")\n",
    "\n",
    "        # 2. Perform validation on the validation test by the risk\n",
    "        # risk_this_epoch = 0\n",
    "        # for c in range(int(np.ceil(N_val / batch_size))):\n",
    "        #     X_val_batch = X_val[c * batch_size: (c + 1) * batch_size]\n",
    "        #     y_val_batch = y_val[c * batch_size: (c + 1) * batch_size]\n",
    "        #     _, _, risk = predict(X_val_batch, w, y_val_batch)\n",
    "        #     risk_this_epoch += risk\n",
    "        # TODO: FIXED: Validation set should not be run in batches\n",
    "        # y, t_hat, _, acc, grad = predict(X_val, w, t_val)\n",
    "        acc_val.append(val_acc)\n",
    "        \n",
    "        # 3. Keep track of the best validation epoch, risk, and the weights\n",
    "        if acc_val[epoch] <= acc_best:\n",
    "            epoch_best = epoch\n",
    "            acc_best = acc_val[epoch]\n",
    "            w_best = w\n",
    "\n",
    "    # Return some variables as needed\n",
    "    return epoch_best, losses_train, acc_val, acc_best, w\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(50000, 785) (50000, 1) (10000, 785) (10000, 1) (10000, 785) (10000, 1)\n",
      "epoch 0/50 loss: 3.43348988772767\n",
      "epoch 1/50 loss: 2.855497767749721\n",
      "epoch 2/50 loss: 2.8302413001285185\n",
      "epoch 3/50 loss: 2.8239134324676134\n",
      "epoch 4/50 loss: 2.8219290899883727\n",
      "epoch 5/50 loss: 2.821256890637744\n",
      "epoch 6/50 loss: 2.821021443982034\n",
      "epoch 7/50 loss: 2.8209376301566107\n",
      "epoch 8/50 loss: 2.820907554554725\n",
      "epoch 9/50 loss: 2.8208967282853337\n",
      "epoch 10/50 loss: 2.820892834218276\n",
      "epoch 11/50 loss: 2.820891440850728\n",
      "epoch 12/50 loss: 2.8208909480254887\n",
      "epoch 13/50 loss: 2.8208907775661145\n",
      "epoch 14/50 loss: 2.820890721075388\n",
      "epoch 15/50 loss: 2.820890703936811\n",
      "epoch 16/50 loss: 2.820890699786247\n",
      "epoch 17/50 loss: 2.820890699537518\n",
      "epoch 18/50 loss: 2.8208907002048567\n",
      "epoch 19/50 loss: 2.82089070090068\n",
      "epoch 20/50 loss: 2.8208907014236893\n",
      "epoch 21/50 loss: 2.8208907017743083\n",
      "epoch 22/50 loss: 2.820890701996963\n",
      "epoch 23/50 loss: 2.820890702134309\n",
      "epoch 24/50 loss: 2.820890702217625\n",
      "epoch 25/50 loss: 2.8208907022676675\n",
      "epoch 26/50 loss: 2.8208907022975462\n",
      "epoch 27/50 loss: 2.8208907023153182\n",
      "epoch 28/50 loss: 2.8208907023258694\n",
      "epoch 29/50 loss: 2.8208907023321204\n",
      "epoch 30/50 loss: 2.8208907023358254\n",
      "epoch 31/50 loss: 2.8208907023380196\n",
      "epoch 32/50 loss: 2.820890702339319\n",
      "epoch 33/50 loss: 2.8208907023400904\n",
      "epoch 34/50 loss: 2.820890702340545\n",
      "epoch 35/50 loss: 2.820890702340817\n",
      "epoch 36/50 loss: 2.820890702340973\n",
      "epoch 37/50 loss: 2.820890702341071\n",
      "epoch 38/50 loss: 2.820890702341127\n",
      "epoch 39/50 loss: 2.8208907023411625\n",
      "epoch 40/50 loss: 2.8208907023411776\n",
      "epoch 41/50 loss: 2.8208907023411935\n",
      "epoch 42/50 loss: 2.8208907023411998\n",
      "epoch 43/50 loss: 2.820890702341201\n",
      "epoch 44/50 loss: 2.820890702341204\n",
      "epoch 45/50 loss: 2.820890702341206\n",
      "epoch 46/50 loss: 2.820890702341207\n",
      "epoch 47/50 loss: 2.820890702341209\n",
      "epoch 48/50 loss: 2.8208907023412086\n",
      "epoch 49/50 loss: 2.820890702341211\n",
      "1003.8532\n",
      "At epoch 0 val:  0 train: [1004.0457, 1003.9064, 1003.8604, 1003.8791, 1003.864, 1003.8606, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532, 1003.8532]\n",
      "The accuracy best epoch with test set is 0.0000000000\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(X_train.shape, t_train.shape, X_val.shape, t_val.shape, X_test.shape,\n",
    "      t_test.shape)\n",
    "\n",
    "\n",
    "epoch_best, losses_train, acc_val, acc_best, W_best = train(X_train, t_train, X_val, t_val)\n",
    "\n",
    "\n",
    "print('At epoch', epoch_best, 'val: ', acc_best, 'train:', acc_val)\n",
    "print(f\"The accuracy best epoch with test set is {acc_best:10.10f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The learning curve of the training loss, where x-axis is the number of epochs,\n",
    "and y-axis is the training loss."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# part a.plot.1\n",
    "fig = plt.figure()\n",
    "plt.plot(losses_train, label=\"Training Losses\")\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()\n",
    "plt.xlabel('Number of epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Doing predictions on the test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_test, t_hat_test, loss_test, acc_test, grad = predict(X_test, W_best, t_test)\n",
    "\n",
    "print('At epoch', epoch_best, 'val: ', acc_best, 'test:', acc_test)\n",
    "print(f\"The loss of best epoch with test data is  {loss_test:10.10f}\")\n",
    "print(f\"The accuracy best epoch with test set is {acc_test:10.10f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[7 2 4 ... 1 1 3] [7 2 2 ... 1 1 3]\n",
      "(10000,) (10000,)\n",
      "8959\n",
      "1041\n",
      "accuracy:0.8959\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def getProbsAndPreds(someX, w):\n",
    "    probs = softmax(np.dot(someX, w))\n",
    "    preds = np.argmax(probs,axis=1)\n",
    "    return probs,preds\n",
    "# inspired by https://medium.com/@awjuliani/simple-softmax-in-python-tutorial-d6b4c4ed5c16\n",
    "# https://www.w3resource.com/numpy/manipulation/ndarray-flatten.php\n",
    "def getAccuracy(someX,someY,w):\n",
    "    prob,prede = getProbsAndPreds(someX,w)\n",
    "    someY = someY.flatten()\n",
    "    print(prede, someY)\n",
    "    print(prede.shape, someY.shape)\n",
    "    num_correct = np.sum(prede == someY)\n",
    "    num_incorrect = np.sum(prede != someY)\n",
    "    print(num_correct)\n",
    "    print(num_incorrect)\n",
    "    accuracy = num_correct/(num_correct+num_incorrect)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "print(f\"accuracy:{getAccuracy(X_val, t_val, W_best)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}